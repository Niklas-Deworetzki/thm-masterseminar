
\section{Partial Evaluation}\label{sec:partial-evaluation}

In this chapter, we dive deeper into the inner workings and implementation details of partial evaluators.
The previous chapters already covered the operational mode of partial evaluators on a larger scale.
Thus, we already know that a partial evaluator accepts a source program as well as some fixed inputs to the source program as its own input and produces a residual output consisting of computations that cannot be evaluated under the given static input.

During specialization a partial evaluator will analyze the components of a program, that represent computations.
While the specific components are dependent on the programming language used for the input program, a partial evaluator will usually work on expressions and statements, as well as the definitions accompanying them.
Consequently a partial evaluator manages information about known names (such as the fixed inputs) and transforms the present components during specialization.
This way, redundant computations are removed or the structure of statements or expressions is simplified using the known information.

This chapter will give an overview of different methods, that can be used to perform computations given some static input.
These methods act as instruments for a partial evaluator, to optimize a program during specialization.
Afterwards it is explained, how a partial evaluator can decide, which computations can be performed statically and which have to be present in the residual program.


\subsection{Instruments of Partial Evaluation}\label{sec:pe-instruments}

In the following, we will see different available techniques, that allow partial evaluators to compute static values or expressions containing (at least some) static values.
Partial evaluation is mainly based on the propagation and removal of statically known data.
Since initially only parts of the input of a source program are known as static data, this is the only way a partial evaluator is able to analyze and optimize an entire program.
\cite[Chap. 1]{Jones_PartialEvaluation}~mentions three main techniques, which are discussed further next: symbolic computation, unfolding of function calls and program point specialization.

\subsubsection*{Symbolic Computation}

Symbolic computation can be used as an optimization technique in a partial evaluator.
The essence of symbolic computation as an optimization technique is to use the structure of an expression, to simplify and thus optimize it.
Under this aspect, we also consider performed computations as well as constant folding, constant propagation and their subfields (e.g.\ sparse constant propagation, which is based on conditionals).
Symbolic computation represents a powerful technique since it is not only possible to evaluate completely constant expressions, but also simplify expressions that are based on constants.

As an example consider Listing~\ref{lst:symbolic-computation}.
If the input \texttt{x} of function \texttt{f} was fixed as \texttt{x = 1}, a partial evaluator could generate a specialized function \texttt{f1} as part of a residual program.
The original function contains two multiplications, one addition as well as one division and one subtraction, while the specialized function contains only a single multiplication operation.
This optimization is achieved by applying the aforementioned patterns:

\begin{enumerate}
\item Replacing the variable \texttt{x} with the fixed value of \texttt{1} yields the following expression as an intermediate result: \texttt{y / (1 - 2) + 3 * 1 * y}.
\item The constant subexpressions \texttt{3 * 1} and \texttt{1 - 2} can be evaluated using constant folding, resulting in the new expression \texttt{y / (-1) + 3 * y}.
\item The division \texttt{y / (-1)} can be simplified to \texttt{-y} since \texttt{1} represents the neutral element of division, resulting in the expression \texttt{-y + 3 * y}.
\item Finally symbolic computation allows us to simplify this expression into \texttt{2 * y}.
\end{enumerate}

\begin{lstlisting}[language=scala,caption={Definition of a simple function and its specialization.},label={lst:symbolic-computation}]
  def f(x: Int, y: Int): Int =
    y / (x - 2) + 3 * x * y

  def f1(y: Int): Int =
    2 * y
\end{lstlisting}


\subsubsection*{Unfolding Function Calls}

The unfolding of function calls (also known as \textit{inlining}) is a comparatively simple technique, that allows for optimizations to be applied beyond limits of a single function.
When unfolding a function call, the body of the called function is simply inserted at call position renaming occurring variables according to the function's parameters.

Listing~\ref{lst:unfolding} shows an example of a function that is specialized and optimized by unfolding its recursive calls.
Unfolding introduces the whole expression inside the function's body into the initial expression, where the parameter is consistently renamed with\linebreak \texttt{(n - 1)}.
This introduces another recursive call with parameter \texttt{(n - 2)}, which unfolded introduces another recursive call, etc.

If applied without care, this scheme can result in infinite loops.
While in this example it is possible to terminate the recursive unfolding if the condition within the recursive expression is evaluated, more complex functions are not guaranteed to terminate.
This example also shows, that optimization schemes can be combined to yield even more optimized results, as the resulting expression could be reduced even further using constant folding.

\begin{lstlisting}[language=scala,caption={Definition of the \texttt{factorial} function and its specialization.},label={lst:unfolding}]
  def factorial(n: Int): Int =
    if (n == 0) 1
    else n * factorial(n - 1)

  def factorial3(): Int =
    3 * (3 - 1) * (3 - 2) * 1
\end{lstlisting}


\subsubsection*{Program Point Specialization}

Program point specialization describes a technique to specialize functions as part of a larger program.
Especially in larger programs, a function may be called multiple times with the same fixed partial inputs.
Instead of generating a new function definition for each occurrence or unfolding every call, it may be preferred to extract a shared function definition to minimize code size.
Listing~\ref{lst:pps} shows the specialization of Ackermann's function with the fixed input \texttt{n = 2} as an example, while also applying the previously mentioned optimization strategies.

\begin{lstlisting}[language=scala,caption={Definition of the \texttt{ackermann} function and its specialization.},label={lst:pps}]
  def ack(n: Int, m: Int): Int =
    if (m == 0) n + 1
    else if (n == 0) a(m - 1, 1)
    else a(m-1, a(m, n-1))

  def ack2(n: Int): Int =
    if (n == 0) 3
    else a1(a2(n - 1))

  def ack1(n: Int): Int =
    if (n == 0) 2
    else a1(n - 1)
\end{lstlisting}



\subsection{Offline and Online Partial Evaluation}\label{sec:offline-vs-online}

As it was shown, a partial evaluator has to differentiate between static and dynamic parts of a program.
If some expression or statement only depends on static data, the computation represented by these elements is static too and can be performed during evaluation.
On the other hand it is possible, that some parts may depend on input data that is only known at runtime.
In this case, it specialization is not always possible and the computations have to be included in the residual program.

To properly perform the specialization, a partial evaluator must divide all parts of the input program into either static or dynamic computations.
The way this division is made is the key difference between the two main variants of partial evaluators.
This variants differentiate between \textit{online} and \textit{offline} partial evaluation, each of which has its own advantages and disadvantages.

\paragraph{Offline Partial Evaluation}
depends on annotations in the source code in order to\linebreak decide, whether a computation can be performed during specialization or has to be generated as part of the residual program.
These annotations can either be provided by the programmer or are inserted automatically during a preprocessing phase of specialization.
During so-called binding-time analysis, the program is analyzed and annotations are propagated.
This way of dividing a source program is usually rather conservative.
Annotations are only based on the knowledge of what input variables are fixed and without knowledge of the inputs values~\cite[Chap. 7]{Jones_PartialEvaluation}.
After the division is complete, all computations annotated as static are performed, while those annotated as dynamic are generated as part of the residual program.


\paragraph{Online Partial Evaluation}
is more of an \enquote{on the fly}-approach to partial evaluation.
The division between static and dynamic is made during specialization when expressions or statements are encountered in an online partial evaluator.
This way, partial evaluation is more complex, since the overhead of creating a division is not factored out as a separate preprocessing step~\cite{Cook_TutorialOnlinePartialEvaluation}.
Consequently, it is harder to predict the speedup online partial evaluation can achieve.
During traversal of the source program, an online partial evaluator has information about the values of different inputs, variables and expressions and thus can often perform more optimizations compared to an offline evaluator.
On the other hand, a (recursive) definition could be encountered many times leading to many or potentially infinite versions of it known to the evaluator.
Not only does this run the risk of generating a huge residual program, but it can also lead to the partial evaluator not terminating if these cases are not accounted for.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../paper"
%%% End:
