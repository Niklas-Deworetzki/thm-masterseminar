
\section{Partial Evaluation}\label{sec:partial-evaluation}

In this chapter, we dive deeper into the inner workings and implementation details of partial evaluators.
The previous chapters already covered the operational mode of partial evaluators on a larger scale.
Thus, we already know that a partial evaluator accepts a source program as well as some fixed inputs to the source program as its own input and produces a residual output consisting of computations that cannot be evaluated under the given static input.

During specialization a partial evaluator will analyze the components of a program, that represent computations.
While the specific components are dependent on the programming language used for the input program, a partial evaluator will usually work on expressions and statements, as well as the definitions accompanying them.
Consequently a partial evaluator manages information about known names (such as the fixed inputs) and transforms the present components during specialization.
This way, redundant computations are removed or the structure of statements or expressions is simplified using the known information.

This chapter will give an overview of different methods, that can be used to perform computations given some static input.
These methods act as instruments for a partial evaluator, to optimize a program during specialization.
Afterwards it is explained, how a partial evaluator can decide, which computations can be performed statically and which have to be present in the residual program.


\subsection{Instruments of Partial Evaluation}\label{sec:pe-instruments}

In the following, we will see different techniques that allow partial evaluators to perform computations if they depend on (at least some) static values.
Since a partial evaluator can only work on the statically known information, most techniques are based on the propagation of statically known information or the removal of redundant computations.
\cite[Chap. 1]{Jones_PartialEvaluation}~mentions three main techniques, which are discussed further next: symbolic computation, unfolding of function calls and program point specialization.

\subsubsection*{Symbolic Computation}

Symbolic computation can be used as an optimization technique in a partial evaluator.
The essence of symbolic computation as an optimization technique is to perform computations on symbols themselves instead of their values.
This allows the simplification of expressions, restructuring of control flow and other transformations, that aim to reduce the computations required to perform at runtime.
Under the aspect of symbolic computation, we also consider constant folding, constant propagation and their subfields (e.g.\ sparse constant propagation, which is based on conditionals).
Symbolic computation presents a powerful technique, since it is not only possible to compute completely static values but also simplify computations based on constants or computations with a known structure.

\begin{figure}[h]
  \setcounter{figure}{0}
  \setcounter{lstlisting}{1}
  \renewcommand\figurename{Listing}
\noindent\begin{minipage}{.45\textwidth}
  \begin{lstlisting}[language=c,autogobble=true]
    void p(int x, int y) {
      print(x);
      if (y == 0) x += y;
      print(2 * x);
    }
  \end{lstlisting}
\end{minipage}\hfill
\begin{minipage}{.45\textwidth}
  \begin{lstlisting}[language=c,autogobble=true]
    void p1(/* x is 1 */ int y) {
      print(1);

      print(2);
    }
  \end{lstlisting}
\end{minipage}
  \caption{Definition of a simple procedure and its specialization}\label{lst:symbolic-computation}
\end{figure}


As an example consider Listing~\ref{lst:symbolic-computation} and its specialization with a known input of \texttt{x = 1}.
The partial evaluator could do sparse constant propagation to replace all occurrences of \texttt{y} within the branch with \texttt{0}, since this is the only case, the branch is taken.
Then it could eliminate the addition of \texttt{0} to the value of \texttt{x}, since \texttt{0} is the neutral element of addition and thus has no effect.
Afterwards the whole branch could be eliminated, since its body has no effect and thus the branch is not necessary.
Finally the expression \texttt{2 * x} could be reduced to the static value of \texttt{2}, since the value of \texttt{x} is known to be \texttt{1}.

\subsubsection*{Unfolding Function Calls}

The unfolding of function calls (also known as \textit{inlining}) is a comparatively simple technique, that allows for optimizations to be applied beyond limits of a single function.
When unfolding a function call, the body of the called function is simply inserted at call position renaming occurring variables according to the function's parameters.

Listing~\ref{lst:unfolding} shows an example of a function that is specialized and optimized by unfolding its recursive calls.
Unfolding introduces the whole expression inside the function's body into the initial expression, where the parameter is consistently renamed with\linebreak \texttt{(n - 1)}.
This introduces another recursive call with parameter \texttt{(n - 2)}, which unfolded introduces another recursive call, etc.

If applied without care, this scheme can result in infinite loops.
While in this example it is possible to terminate the recursive unfolding if the condition within the recursive expression is evaluated, more complex functions are not guaranteed to terminate.
This example also shows, that optimization schemes can be combined to yield even more optimized results, as the resulting expression could be reduced even further using constant folding.

\newpage

\begin{lstlisting}[language=scala,caption={Definition of the \texttt{factorial} function and its specialization.},label={lst:unfolding}]
  def factorial(n: Int): Int =
    if (n == 0) 1
    else n * factorial(n - 1)

  def factorial3(/* n is 3 */): Int =
    3 * (3 - 1) * (3 - 2) * 1
\end{lstlisting}


\subsubsection*{Program Point Specialization}

Program point specialization describes a technique to specialize functions as part of a larger program.
Especially in larger programs, a function may be called multiple times with the same fixed partial inputs.
Instead of generating a new function definition for each occurrence or unfolding every call, it may be preferred to extract a shared function definition to minimize code size.
Listing~\ref{lst:pps} shows the specialization of Ackermann's function with the fixed input \texttt{m = 2} as an example, while also applying the previously mentioned optimization strategies.

\begin{lstlisting}[language=scala,caption={Definition of the \texttt{ackermann} function and its specialization.},label={lst:pps}]
  def ack(m: Int, n: Int): Int =
    if (m == 0) n + 1
    else if (n == 0) ack(m - 1, 1)
    else ack(m-1, a(m, n-1))

  def ack2(/* m is 2 */ n: Int): Int =
    if (n == 0) 3
    else ack1(ack2(n - 1))

  def ack1(/* m is 1 */ n: Int): Int =
    if (n == 0) 2
    else ack1(n - 1) + 1
\end{lstlisting}



\subsection{Offline and Online Partial Evaluation}\label{sec:offline-vs-online}

As it was shown, a partial evaluator has to differentiate between static and dynamic parts of a program.
If some expression or statement only depends on static data, the computation represented by these elements is static too and can be performed during evaluation.
On the other hand it is possible, that some parts may depend on input data that is only known at runtime.
In this case, it specialization is not always possible and the computations have to be included in the residual program.

To properly perform the specialization, a partial evaluator must divide all parts of the input program into either static or dynamic computations.
The way this division is made is the key difference between the two main variants of partial evaluators.
This variants differentiate between \textit{online} and \textit{offline} partial evaluation, each of which has its own advantages and disadvantages.

\paragraph{Offline Partial Evaluation}
depends on annotations in the source code in order to\linebreak decide, whether a computation can be performed during specialization or has to be generated as part of the residual program.
These annotations can either be provided by the programmer or are inserted automatically during a preprocessing phase of specialization.
During so-called binding-time analysis, the program is analyzed and annotations are propagated.
This way of dividing a source program is usually rather conservative.
Annotations are only based on the knowledge of what input variables are fixed and without knowledge of the inputs values~\cite[Chap. 7]{Jones_PartialEvaluation}.
After the division is complete, all computations annotated as static are performed, while those annotated as dynamic are generated as part of the residual program.


\paragraph{Online Partial Evaluation}
is more of an \enquote{on the fly}-approach to partial evaluation.
The division between static and dynamic is made during specialization when expressions or statements are encountered in an online partial evaluator.
This way, partial evaluation is more complex, since the overhead of creating a division is not factored out as a separate preprocessing step~\cite{Cook_TutorialOnlinePartialEvaluation}.
Consequently, it is harder to predict the speedup online partial evaluation can achieve.
During traversal of the source program, an online partial evaluator has information about the values of different inputs, variables and expressions and thus can often perform more optimizations compared to an offline evaluator.
On the other hand, a (recursive) definition could be encountered many times leading to many or potentially infinite versions of it known to the evaluator.
Not only does this run the risk of generating a huge residual program, but it can also lead to the partial evaluator not terminating if these cases are not accounted for.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../paper"
%%% End:
